{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/en0/jss548/miniconda3/envs/radar-dev/lib/python3.8/site-packages/distributed/node.py:240: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37319 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#PBS -q express\n",
      "#PBS -l walltime=10:00:00\n",
      "#PBS -P kl02\n",
      "#PBS -l ncpus=32\n",
      "#PBS -l walltime=10:00:00,mem=64GB\n",
      "#PBS -l jobfs=100GB\n",
      "#PBS -l storage=gdata/rq0+gdata/kl02+gdata/hj10+scratch/kl02+gdata/en0+gdata/lb4\n",
      "#PBS -l wd\n",
      "#PBS -m ab\n",
      "export PATH=\"/g/data/en0/jss548/miniconda3:$PATH\"\n",
      "source activate radar-dev\n",
      "/g/data/en0/jss548/miniconda3/envs/radar-dev/bin/python -m distributed.cli.dask_worker tcp://10.6.45.57:40885 --nthreads 32 --memory-limit 64.00GB --name name --nanny --death-timeout 60 --local-directory /scratch/kl02/jss548/dask-cluster\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "dask.config.set({\"distributed.comm.timeouts.tcp\": \"50s\"}) #increase to prevent timeout when waiting for scheduler job to start\n",
    "\n",
    "#delete cluster if it's running\n",
    "try:\n",
    "    cluster.close()\n",
    "    del cluster\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#start cluster scheduler\n",
    "cluster = PBSCluster(\n",
    "                    # Dask-worker specific keywords\n",
    "                    shebang='#!/bin/bash',\n",
    "                    cores=32,\n",
    "                    processes=1,\n",
    "                    memory='64GB',\n",
    "                    local_directory='/scratch/kl02/jss548/dask-cluster',  # Location to put temporary data if necessary\n",
    "                    header_skip=['select','dask-worker'],\n",
    "                    env_extra=['export PATH=\"/g/data/en0/jss548/miniconda3:$PATH\"','source activate radar-dev'],\n",
    "                    # Job scheduler specific keywords\n",
    "                    queue='express',\n",
    "                    walltime='10:00:00',\n",
    "                    job_extra=['-P kl02',\n",
    "                               '-l ncpus=32',\n",
    "                               '-l walltime=10:00:00,mem=64GB',\n",
    "                               '-l jobfs=100GB',\n",
    "                               '-l storage=gdata/rq0+gdata/kl02+gdata/hj10+scratch/kl02+gdata/en0+gdata/lb4',\n",
    "                               '-l wd',\n",
    "                               '-m ab'])\n",
    "\n",
    "\n",
    "#print PBS for workers\n",
    "print(cluster.job_script())\n",
    "#start 8 workers\n",
    "cluster.scale(jobs=8)\n",
    "#connect to scheduler\n",
    "client = Client(cluster)\n",
    "#run dask bag job here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'closed', 'job_cls': <class 'dask_jobqueue.pbs.PBSJob'>, '_kwargs': {'cores': 32, 'processes': 1, 'memory': '64GB', 'local_directory': '/scratch/kl02/jss548/dask-cluster', 'project': 'kl02', 'queue': 'normal', 'walltime': '10:00:00', 'resource_spec': 'select=1:ncpus=32:mem=100GB', 'job_extra': ['-l storage=gdata/rq0+gdata/kl02+gdata/hj10+scratch/kl02+gdata/en0+gdata/lb4', '-l wd', '-l jobfs=100GB'], 'config_name': 'pbs', 'interface': None, 'protocol': 'tcp://', 'security': None}, '_created': set(), 'scheduler_spec': {'cls': <class 'distributed.scheduler.Scheduler'>, 'options': {'protocol': 'tcp://', 'dashboard_address': ':8787', 'security': None, 'interface': None}}, 'worker_spec': {}, 'new_spec': {'cls': <class 'dask_jobqueue.pbs.PBSJob'>, 'options': {'cores': 32, 'processes': 1, 'memory': '64GB', 'local_directory': '/scratch/kl02/jss548/dask-cluster', 'project': 'kl02', 'queue': 'normal', 'walltime': '10:00:00', 'resource_spec': 'select=1:ncpus=32:mem=100GB', 'job_extra': ['-l storage=gdata/rq0+gdata/kl02+gdata/hj10+scratch/kl02+gdata/en0+gdata/lb4', '-l wd', '-l jobfs=100GB'], 'config_name': 'pbs', 'interface': None, 'protocol': 'tcp://', 'security': None}}, 'workers': {}, '_i': 0, 'security': Security(require_encryption=False), 'scheduler_comm': <rpc to 'tcp://10.6.45.57:38351', 0 comms>, '_futures': set(), '_old_logging_level': 40, '_old_bokeh_logging_level': 40, '_loop_runner': <distributed.utils.LoopRunner object at 0x145faccdee80>, 'loop': <zmq.eventloop.ioloop.ZMQIOLoop object at 0x145faccde1c0>, '_correct_state_waiting': None, '_name': 'PBSCluster', 'scheduler_info': {'type': 'Scheduler', 'id': 'Scheduler-424e025c-cf04-4107-ae55-6ca63f6fe86e', 'address': 'tcp://10.6.45.57:38351', 'services': {'dashboard': 43495}, 'workers': {}}, 'periodic_callbacks': {}, '_asynchronous': False, '_lock': <asyncio.locks.Lock object at 0x145facce79d0 [unlocked]>, 'scheduler': <Scheduler: \"tcp://10.6.45.57:38351\" processes: 0 cores: 0>, '_watch_worker_status_comm': <closed TCP>, '_watch_worker_status_task': <Task finished name='Task-44' coro=<Cluster._watch_worker_status() done, defined at /g/data/en0/jss548/miniconda3/envs/radar-dev/lib/python3.8/site-packages/distributed/deploy/cluster.py:91> result=None>}\n"
     ]
    }
   ],
   "source": [
    "cluster.close()\n",
    "print(vars(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#PBS -N dask-worker\n",
      "#PBS -q normal\n",
      "#PBS -A kl02\n",
      "#PBS -l select=1:ncpus=32:mem=60GB\n",
      "#PBS -l walltime=10:00:00\n",
      "\n",
      "/g/data/en0/jss548/miniconda3/envs/radar-dev/bin/python -m distributed.cli.dask_worker tcp://10.6.45.57:34785 --nthreads 32 --memory-limit 64.00GB --name name --nanny --death-timeout 60 --local-directory $TMPDIR\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:radar-dev] *",
   "language": "python",
   "name": "conda-env-radar-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
